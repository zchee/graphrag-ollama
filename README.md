# GraphRAG

ğŸ‘‰ [Use the GraphRAG Accelerator solution](https://github.com/Azure-Samples/graphrag-accelerator) <br/>
ğŸ‘‰ [Microsoft Research Blog Post](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)<br/>
ğŸ‘‰ [Read the docs](https://microsoft.github.io/graphrag)<br/>
ğŸ‘‰ [GraphRAG Arxiv](https://arxiv.org/pdf/2404.16130)

<div align="left">
  <a href="https://pypi.org/project/graphrag/">
    <img alt="PyPI - Version" src="https://img.shields.io/pypi/v/graphrag">
  </a>
  <a href="https://pypi.org/project/graphrag/">
    <img alt="PyPI - Downloads" src="https://img.shields.io/pypi/dm/graphrag">
  </a>
  <a href="https://github.com/microsoft/graphrag/issues">
    <img alt="GitHub Issues" src="https://img.shields.io/github/issues/microsoft/graphrag">
  </a>
  <a href="https://github.com/microsoft/graphrag/discussions">
    <img alt="GitHub Discussions" src="https://img.shields.io/github/discussions/microsoft/graphrag">
  </a>
</div>

## Overview

The GraphRAG project is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs.

To learn more about GraphRAG and how it can be used to enhance your LLM's ability to reason about your private data, please visit the <a href="https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/" target="_blank">Microsoft Research Blog Post.</a>

åˆ†æ”¯feature/ollama-supportä¸Šå®ç°äº†æ”¯æŒOllamaå…è´¹å¤§æ¨¡å‹çš„è°ƒç”¨ï¼Œæœ¬åœ°ä¹Ÿå¯ä»¥ç©äº†ã€‚

è·Ÿç€[æ•™ç¨‹](https://microsoft.github.io/graphrag/get_started/)èµ°ä¸€é

settings.yamlå‚è€ƒé…ç½®å¦‚ä¸‹ï¼š

llm:

type: ollama_chat # or azure_openai_chat

model: llama3.1:8b

model_supports_json: false # recommended if this is available for your model.

max_tokens: 12800

api_base: http://localhost:11434

concurrent_requests: 2 # the number of parallel inflight requests that may be made

embeddings:

llm:

type: ollama_embedding # or azure_openai_embedding

model: nomic-embed-text:latest

api_base: http://localhost:11434

concurrent_requests: 2 # the number of parallel inflight requests that may be made
